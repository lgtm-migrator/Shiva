import Algorithm
import ReplayBuffer
import Environment
from abc import ABC
import subprocess
from tensorboardX import SummaryWriter


class AbstractLearner(ABC):

    def __init__(self,
                agents : list,
                environments : list,
                algorithm : str,
                data : list,
                configs: dict,
                id : "An id generated by metalearner to index the learner.",
                root
                ):

        self.agents = agents
        self.environments = environments
        self.algorithm = algorithm
        self.data = None
        self.agentCount = 0

    def update(self):
        pass

    def step(self):
        pass

    def create_env(self, alg):
        pass

    def get_agents(self):
        pass

    def get_algorithm(self):
        pass

    def launch(self):
        pass

    def save_agent(self):
        pass

    def load_agent(self):
        pass

    def id_generator(self):
        id = self.agentCount
        self.agentCount +=1
        return id


###########################################################################
# 
#               Single Agent Learner
#         
###########################################################################

'''

    This Learner should be able to handle DQN and DDPG algorithms.

'''

class Single_Agent_Learner(AbstractLearner):

    def __init__(self, 
                agents, 
                environments, 
                algorithm, 
                data, 
                configs, 
                learner_id,
                root):

        super().__init__(
                        agents, 
                        environments, 
                        algorithm, 
                        data, 
                        configs, 
                        learner_id,
                        root)

        self.configs = configs[0]
        self.id = learner_id
        self.root = root

        #I'm thinking about getting the saveFrequency here from the config and saving it in self
        self.saveFrequency = configs[0]['Learner']['save_frequency']


    def update(self):

        for ep_count in range(self.configs['Learner']['episodes']):

            self.env.reset()

            self.totalReward = 0

            done = False
            while not done:
                done = self.step(ep_count)
                self.writer.add_scalar('Loss', self.alg.get_loss(), self.env.get_current_step())

        # make an environment close function
        # self.env.close()
        self.env.env.close()

    # Function to step throught the environment
    def step(self,ep_count):


        observation = self.env.get_observation()

        action = self.alg.get_action(self.alg.agents[0], observation, self.env.get_current_step())

        next_observation, reward, done = self.env.step(action)

        # Write to tensorboard
        self.writer.add_scalar('Reward', reward, self.env.get_current_step())

        # Cumulate the reward
        self.totalReward += reward[0]

        self.buffer.append([observation, action, reward, next_observation, done])

        self.alg.update(self.agents, self.buffer.sample(), self.env.get_current_step())

        # when the episode ends
        if done:
            # add values to the tensorboard
            self.writer.add_scalar('Total Reward', self.totalReward, ep_count)
            self.writer.add_scalar('Average Loss per Episode', self.alg.get_average_loss(self.env.get_current_step()), ep_count)


        # Save the model periodically
        if self.env.get_current_step() % self.saveFrequency == 0:
            pass

        return done

    def create_environment(self):
        # create the environment and get the action and observation spaces
        self.env = Environment.initialize_env(self.configs['Environment'])


    def get_agents(self):
        return self.agents[0]

    def get_algorithm(self):
        return self.algorithm

    # Initialize the model
    def launch(self):

        self.root = self.makeDirectory(self.root)

        # Launch the environment
        self.create_environment()

        # Launch the algorithm which will handle the
        self.alg = Algorithm.initialize_algorithm(self.env.get_observation_space(), self.env.get_action_space(), [self.configs['Algorithm'], self.configs['Agent'], self.configs['Network']])

        #self.agents = self.alg.create_agent(self.root, self.id_generator())
        self.agents = self.load_agent('MountainCar_Agent_300_57151.pth', self.configs)

        log_dir = "{}/Agents/{}/logs".format(self.root, self.agents.id)

        print("\nHere's the directory to the tensorboard output\n",log_dir)

        self.writer =  SummaryWriter(log_dir)

        # Basic replay buffer at the moment
        self.buffer = ReplayBuffer.initialize_buffer(self.configs['ReplayBuffer'], 1, self.env.get_action_space(), self.env.get_observation_space())

    def save_agent(self):
        self.alg.agents[0].save(self.env.get_current_step())

    # do this for travis
    def load_agent(self, path, configs):
        # first I need to create an agent in alg
        # then I need to overwrite it
        self.alg.agents[0].load(path, self.configs)


    def makeDirectory(self, root):
    
        # make the learner folder name
        root = root + '/learner{}'.format(self.id)
        
        # make the folder
        subprocess.Popen("mkdir " + root, shell=True)
        
        # return root for reference  
        return root




class Single_Agent_Imitation_Learner(AbstractLearner):
    def __init__(self):

        super(ImitationLearner,self).__init(agents,environments,algorithm,data,configs)
        self.agents = agents
        self.environments = environments
        self.algorithm = algorithm
        self.data = data
        self.configs = configs

    def create_environment(self):
        #Initialize environment-config file will specify which environment type
        self.env = Environment.initialize_env(self.configs['Environment'])

    def get_agents(self):
        return self.agents[0]

    def get_algorithm(self):
        return self.Algorithm

    def launch(self):
        pass