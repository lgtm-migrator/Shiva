Updates:
1. Changed exploration to use exploration.scale to be the eps for eps greedy for the discrete actions 
and as the OUNoise scale for the params. This is significant because OUNoise on the whole action space was clamping the parameters and the discrete logits (-1,1)

2. Hauschknet uses 500 time steps per ep.


Questions:

1. In maddpg.py do we want to use grad norm clipping?
2. Shariq says,
        # Forward pass as if onehot (hard=True) but backprop through a differentiable
            # Gumbel-Softmax sample. The MADDPG paper uses the Gumbel-Softmax trick to backprop
            # through discrete categorical samples, but I'm not sure if that is
            # correct since it removes the assumption of a deterministic policy for
            # DDPG. Regardless, discrete policies don't seem to learn properly without it.
    We are using discrete continuous mixture, but the code calls for one or the other at lines maddpg.py[108 and 137].
    I believe the code is correct as it is passing the whole output of the actor network as is to calc the loss, but I am not sure.
    
3. I guess in pytorch we zero the optimizers gradients every batch?
4. I saw it mentioned somewhere that in our networks' backward function, we should not be modifying the gradient but
    returning a seperate gradient. I'm not sure if we should be doing this.


HFO:
If timesteps is 500, then the episode responds with out of time on timestep 501,
resetting the new episode time step 502 and running out of time error at 1003.