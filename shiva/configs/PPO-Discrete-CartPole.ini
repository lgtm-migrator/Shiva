[MetaLearner]
type = "SingleAgentMetaLearner"
start_mode = "production"
optimize_env_hp = False
optimize_learner_hp = False
evolution = False

[Learner]
type = "SingleAgentPPOLearner"
using_buffer = True
episodes = 1005
load_agents = False
process_count = 5
episodic = True
max_length = 1000
save_checkpoint_episodes = 500

[Algorithm]
algorithm = "PPO"
type = "PPOAlgorithm"
manual_seed = 0
update_episodes = 5
update_epochs = 5
replay_buffer = True
loss_function = "MSELoss"
regularizer = 0
recurrence = 0
gamma = 0.99
lambda = 0.95
beta = 0.001
epsilon_clip = 0.1

[Environment]
type = "GymEnvironment"
env_name = "CartPole-v0"
action_space = "discrete"
observation_space = "continuous"
render = False
num_agents = 1
num_instances = 1
normalize = True
b = 1
a = -1
min = -1
max = 100
seed=0

[Buffer]
type = "SimpleBuffer"
capacity = 100000
batch_size = 128
num_agents = 1

[Agent]
action_space = "Discrete"
manual_seed=0
num_agents = 1
optimizer_function = "Adam"
learning_rate = 0.003
eps = 0.0001

[Network]
actor = {'layers': [128, 128], 'activation_function': ['ReLU', 'ReLU'], 'output_function': None, 'last_layer': True}
critic = {'layers': [128, 128], 'activation_function': ['ReLU', 'ReLU'], 'output_function': None, 'last_layer': True}
policy_base_output = 128

[Admin]
save                = True
traceback           = True
directory           = {'runs': '/runs'}

[ImageProcessing]

[VideoProcessing]
