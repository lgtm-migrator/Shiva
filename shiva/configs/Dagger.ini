;; Test config for Dagger
; NOTE: options within sections cannot have the same names
; some options are repeating, we need to get rid of duplicates

[MetaLearner]
start_mode=                   "production"
type=                         "SingleAgentMetaLearner"
algorithms=                   "Imitation"
eval_env=                     "Gym"
optimize_env_hp =             "False"
optimize_learner_hp =         "False"
evolution =                   "False"

[Evaluation]
env_type =                    "Gym"
environments =                ["CartPole-v1"]
load_path =                   ["runs/"]
metrics =                     ["LossPerEpisode", "RewardPerEpisode"]
env_render =                  True

[Learner]
type=                         'SingleAgentImitationLearner'
using_buffer=                 True
supervised_episodes=          10
imitation_episodes=           5
dagger_iterations=            3
save_frequency=               5000
expert_agent=                 'runs/ML-Breakout-ram-v0-11-01-12:14/L-0/'
;'runs/ML-CartPole-v0-10-25-21:44/'

[Algorithm]
type1=                        'SupervisedAlgorithm'
type2=                        'DaggerAlgorithm'
replay_buffer=                True
learning_rate=                0.01
optimizer=                    'Adam'
loss_function=                'CrossEntropyLoss'
regularizer=                  0
recurrence=                   0
gamma=                        0.99
beta=                         0
epsilon_start=                1
epsilon_end=                  0.02
epsilon_decay=                0.00005
c=                            200

[Environment]
type=                         'GymDiscreteEnvironment'
env_name=                     'Breakout-ram-v0'
action_space=                 'discrete'
observation_space=            "discrete"
render=                       'False'
num_agents=                   2
normalize=                    'False'
b=1
a=-1
min=-1
max=100

[Buffer]
type=                         'SimpleBuffer'
capacity=                     100_000
batch_size=                   16


[Agent]
num_agents=                   2
optimizer_function=           'Adam'
learning_rate=                0.03
action_policy=                'argmax'
;possible values are 'argmax' or 'sample'


[Network]
network = {'layers': [30, 60], 'activation_function':["ReLU","ReLU"], 'output_function': '', 'last_layer': True}

[ImageProcessing]


[VideoProcessing]
