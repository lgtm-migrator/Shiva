[MetaLearner]
type = "SingleAgentMetaLearner"
start_mode = "production"
optimize_env_hp = False
optimize_learner_hp = False
evolution = False

[Learner]
type = "SingleAgentPPOLearner"
using_buffer = True
episodes = 1500
load_agents = False
process_count = 5
max_length = 100000
save_checkpoint_episodes = 1000

[Algorithm]
algorithm = "PPO"
type = "ContinuousPPOAlgorithm"
manual_seed = 1
update_episodes = 20
update_epochs = 10
replay_buffer = True
loss_function = "MSELoss"
regularizer = 0
recurrence = 0
lambda = 0.99
beta = 0.001
epsilon_clip = 0.2
gamma = 0.99

[Environment]
type = "UnityWrapperEnvironment"
exec = "shiva/envs/unitybuilds/3DBall-12_Mac/3DBall.app"
env_name = "3DBall"
discrete_action_size = 2
action_space = False
train_mode = True
render = False
reset_params = {}
port = 5005
update_episodes = 15
normalize = True
on_policy = True

[Buffer]
type = "SimpleBuffer"
capacity = 10024
batch_size = 128
num_agents = 1

[Agent]
action_space = "Continuous"
num_agents = 1
manual_seed = 1
optimizer_function = "Adam"
learning_rate = 0.003
eps = 0.0001

[Network]
mu = {'layers': [128, 128], 'activation_function': ['Tanh', 'Tanh'], 'output_function': 'Tanh', 'last_layer': True}
var = {'layers': [128, 128], 'activation_function': ['ReLU', 'Softplus'], 'output_function': None, 'last_layer': True}
critic = {'layers': [128, 128], 'activation_function': ['ReLU', 'ReLU'], 'output_function': None, 'last_layer': True}
policy_base_output = 128

[Admin]
save = True
traceback = True
directory = {'runs': '/runs'}

[ImageProcessing]

[VideoProcessing]
