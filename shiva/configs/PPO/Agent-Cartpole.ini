[Learner]
episodes = 5000
evaluate = False
load_agents= False
save_checkpoint_episodes = 50
episodes_to_update = 1
n_traj_pulls = 5

evolve = False
initial_evolution_episodes=25
evolution_episodes = 10
p_value = 0.05
perturb_factor = [0.8, 1.2]

[Algorithm]
algorithm = "PPO"
type = "PPOAlgorithm"
manual_seed = 0
update_episodes = 5
update_epochs = 5
replay_buffer = True
loss_function = "MSELoss"
regularizer = 0
recurrence = 0
gamma = 0.99
gae_lambda = 0.99
beta = 0.001
epsilon_clip = 0.2


[Buffer]
type = 'MultiTensorBuffer.MultiAgentTensorBuffer'
capacity = 100000
batch_size = 64
num_agents=1

[Agent]
hp_random = False
lr_range = True
lr_factors = [100,1000,10000,100000,1000000]
lr_uniform = [1,10]
epsilon_range = [0, 0.4]
ou_range = [0,1]

optimizer_function = 'Adam'
critic_learning_rate = 0.002
lr_decay = {'factor': 0.75, 'average_episodes': 50, 'wait_episodes_to_decay': 5}
exploration_steps = 100

epsilon_start = 1
epsilon_end = 0.02
epsilon_decay = 0.00005
epsilon_episodes = 500
epsilon_decay_degree = 2

noise_start = 0.95
noise_end = 0.1
noise_episodes = 500
noise_decay_degree = 2

action_space = "Discrete"
manual_seed=0
num_agents = 1
learning_rate = 0.003
eps = 0.0001

[Network]
actor = {'layers': [128, 128], 'activation_function': ['ReLU', 'ReLU'], 'output_function': None, 'last_layer': True}
critic = {'layers': [128, 128], 'activation_function': ['ReLU', 'ReLU'], 'output_function': None, 'last_layer': True}
policy_base_output = 128
