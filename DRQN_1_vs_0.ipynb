{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested with the following script:\n",
    "# ./bin/HFO --offense-agents=1 --trials 20000 --frames-per-trial 100 --seed 123 --headless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hfo\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.misc\n",
    "import os \n",
    "import csv\n",
    "import itertools \n",
    "import tensorflow.contrib.slim as slim \n",
    "%matplotlib inline\n",
    "\n",
    "#from helper import * \n",
    "\n",
    "\n",
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,12],dtype=tf.float32)\n",
    "          \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        #We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "        #The input must be reshaped into [batch x trace x units] for rnn processing, \n",
    "        #and then returned to [batch x units] when sent through the upper levles.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.scalarInput),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,4]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.scalarInput)\n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,4,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        \n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        \n",
    "        real_sampled_batch_size = batch_size\n",
    "        for episode in sampled_episodes:\n",
    "            try:\n",
    "                point = np.random.randint(0,len(episode)+1- trace_length)\n",
    "                sampledTraces.append(episode[point:point+trace_length])\n",
    "            except ValueError: \n",
    "                print('value error on sampling from replay memory from episode size of ' , len(episode))\n",
    "                real_sampled_batch_size = real_sampled_batch_size - 1\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[real_sampled_batch_size * trace_length,5]) , real_sampled_batch_size\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#Setting the training parameters\n",
    "batch_size = 20  #How many experience traces to use for each training step.\n",
    "trace_length = 10 #How long each experience trace will be when training\n",
    "update_freq = 5 #How often to perform a training step.\n",
    "y = .995 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.08 #Final chance of random action\n",
    "anneling_steps = 10000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 20000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./drqn\" #The path to save our model to.\n",
    "h_size = 12 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 100 #The max allowed length of our episode.\n",
    "time_per_step = 1 #Length of each step used in gif creation\n",
    "summaryLength = 100 #Number of epidoes to periodically save for analysis\n",
    "tau = 0.01 #Rate to update target network toward primary netw\n",
    "\n",
    "\n",
    "   \n",
    "#These functions allows us to update the parameters of our target network with those of the primary network.\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "    total_vars = len(tf.trainable_variables())\n",
    "    a = tf.trainable_variables()[0].eval(session=sess)\n",
    "    b = tf.trainable_variables()[total_vars//2].eval(session=sess)\n",
    "    if a.all() == b.all():\n",
    "        #print(\"Target Set Success\")\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Target Set Failed\")\n",
    "\n",
    "\n",
    "#Record performance metrics and episode logs for the Control Center.\n",
    " \n",
    "        \n",
    "def getReward(s):\n",
    "      reward=0\n",
    "      #--------------------------- \n",
    "      if s=='Goal':\n",
    "        reward=1000\n",
    "      #--------------------------- \n",
    "      elif s=='CapturedByDefense':\n",
    "        reward=-1000\n",
    "      #--------------------------- \n",
    "      elif s=='OutOfBounds':\n",
    "        reward=-1000\n",
    "      #--------------------------- \n",
    "      #Cause Unknown Do Nothing\n",
    "      elif s=='OutOfTime':\n",
    "        reward=-1000\n",
    "      #--------------------------- \n",
    "      elif s=='InGame':\n",
    "        reward=-10\n",
    "      #--------------------------- \n",
    "      elif s=='SERVER_DOWN':  \n",
    "        reward=0\n",
    "      #--------------------------- \n",
    "      else:\n",
    "        print(\"Error: Unknown GameState\", s)\n",
    "        reward = -1\n",
    "      return reward\n",
    "\n",
    "\n",
    "\n",
    "action_list = [hfo.DRIBBLE, hfo.SHOOT, hfo.REORIENT, hfo.GO_TO_BALL ]\n",
    "\n",
    "\n",
    "\n",
    "hfo_env = hfo.HFOEnvironment()\n",
    "\n",
    "hfo_env.connectToServer(hfo.HIGH_LEVEL_FEATURE_SET,\n",
    "                            config_dir=' /HFO-master/bin/teams/base/config/formations-dt', \n",
    "                        server_port=6000, server_addr='localhost', team_name='base_left', play_goalie=False)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#We define the cells for the primary and target q-networks\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main')\n",
    "targetQN = Qnetwork(h_size,cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "ignored_episodes = 0\n",
    "total_steps = 0\n",
    "reward_per_episode = []\n",
    "episode_length = []\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "   \n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        #Reset environment and get first new observation\n",
    "        if i > 0 :\n",
    "            reward_per_episode.append(rAll)\n",
    "            episode_length.append(j)\n",
    "            #if i % 10 ==0 :\n",
    "                #print ('at episode : ', i , 'steps/episode: ', j )\n",
    "                #print ('reward/episode: ', rAll )\n",
    "        s = hfo_env.getState() \n",
    "        d = False \n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size])) #Reset the recurrent layer's hidden state\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "                \n",
    "            #s1P,r,d = env.step(a)\n",
    "            # print ('action chosen to take is ...', a)\n",
    "            hfo_env.act(action_list[a]) # take the action\n",
    "            status = hfo_env.step()\n",
    "            r = getReward(hfo_env.statusToString(status))\n",
    "            s1 = hfo_env.getState() \n",
    "            \n",
    "            if status == hfo.IN_GAME:\n",
    "                d = False\n",
    "            else:\n",
    "                d = True\n",
    "                #print ('end of episode')\n",
    "            #s1 = processState(s1P)\n",
    "            \n",
    "            total_steps += 1\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    #Reset the recurrent layer's hidden state\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                        \n",
    "                    trainBatch , real_batch_size = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    if real_batch_size == batch_size:\n",
    "                        Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "                            mainQN.scalarInput:np.vstack(trainBatch[:,3] ),\\\n",
    "                            mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:real_batch_size})\n",
    "                        Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "                            targetQN.scalarInput:np.vstack(trainBatch[:,3] ),\\\n",
    "                            targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:real_batch_size})\n",
    "                        end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                        doubleQ = Q2[range(real_batch_size*trace_length),Q1]\n",
    "                        targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                        #Update the network with our target values.\n",
    "                        sess.run(mainQN.updateModel, \\\n",
    "                            feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0] ),mainQN.targetQ:targetQ,\\\n",
    "                            mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                            mainQN.state_in:state_train,mainQN.batch_size:real_batch_size})\n",
    "                        #print ('updating the network')\n",
    "\n",
    "                    else:\n",
    "                        print ('not updating the network')\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            #sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        #Add the episode to the experience buffer\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = list(zip(bufferArray))\n",
    "        if len(episodeBuffer) > trace_length:\n",
    "            myBuffer.add(episodeBuffer)\n",
    "        else:\n",
    "            print('ignoring the episode on the replay memory with episode length of ',  len(episodeBuffer)) \n",
    "            ignored_episodes +=1\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        #Periodically save the model. \n",
    "        if i % 200 == 0 and i != 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print ('average reward ', total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "            \n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(rList)\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(jList)\n",
    "            plt.show()\n",
    "\n",
    "            #saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n",
    "             #   summaryLength,h_size,sess,mainQN,time_per_step)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7300 --> 2:31\n",
    "#10420 --> 2:47\n",
    "#15483 --> 3:15\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "modes = ['full', 'same', 'valid']\n",
    "m = modes[2]\n",
    "#for m in modes:\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.convolve( rList, np.ones((50,))/50, mode=m));\n",
    "#plt.axis([-10, 251, -.1, 1.1]);\n",
    "#plt.legend(modes, loc='lower center');\n",
    "\n",
    "plt.savefig('total_reward_per_episode-V1.png')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
